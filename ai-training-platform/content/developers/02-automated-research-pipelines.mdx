---
title: "Automated Research and Data Pipelines"
description: "Learn to build AI-powered research tools that gather and analyse data automatically."
estimatedTime: "60 minutes"
difficulty: "Intermediate"
questions:
  - id: "q1"
    question: "What is the main advantage of using AI for data extraction instead of regex?"
    options:
      - "AI is always faster"
      - "AI can understand context and adapt to layout changes"
      - "AI doesn't need the internet"
      - "AI is free"
    correctAnswer: 1
  - id: "q2"
    question: "What should you always check before scraping a website?"
    options:
      - "The website's colour scheme"
      - "The robots.txt file"
      - "The website's loading speed"
      - "The website's age"
    correctAnswer: 1
  - id: "q3"
    question: "Why is rate limiting important when scraping?"
    options:
      - "It makes your code run faster"
      - "It prevents you from overwhelming the server and being blocked"
      - "It's required by law"
      - "It uses less memory"
    correctAnswer: 1
---

# Automated Research and Data Pipelines

Now that you understand the basics, let's build something useful. This module covers how to create tools that gather and analyse information automatically—essentially giving yourself a research assistant that works around the clock.

![Automated Research](https://media.giphy.com/media/3o7btNa0RUYa5E7iiQ/giphy.gif)

## What You'll Build

By the end of this module, you'll be able to:

- Build simple automated scrapers using AI
- Create data pipelines that feed information into AI for analysis
- Understand the ethical boundaries of web scraping
- Build a basic research engine that runs on a schedule

---

## The Problem We're Solving

Manual research is slow and repetitive.

**The old way:**
- Spend hours searching and copying information
- Lose track of where things came from
- Repeat the whole process when you need updates

**The new way:**
- Run a script
- Get a structured report
- Schedule it to update automatically

As developers, we can build tools that turn "spend 2 hours Googling" into "click button, get report."

![Manual vs Automated](https://media.giphy.com/media/l0HlNQ03J5JxX6lva/giphy.gif)

---

## How a Research Pipeline Works

Think of it like an assembly line:

1. **Ingestion** — Visit websites, call APIs, check RSS feeds
2. **Processing** — Remove noise, organise the raw data
3. **Analysis** — Send it to AI to extract what matters
4. **Storage** — Save it to a database or file
5. **Presentation** — Display it in a useful format

We'll build this step by step. Each piece is straightforward on its own.

![Data Pipeline](https://media.giphy.com/media/xT9IgzoKnwFNmISR8I/giphy.gif)

---

## The Tools

Here's what we'll be working with:

### Puppeteer / Playwright
Controls a browser automatically. Useful for sites that need JavaScript to render content.

### LangChain / LlamaIndex
Frameworks for organising how AI processes information. Makes it easier to structure prompts and handle responses.

### OpenAI API / Anthropic API
The AI that actually analyses the data. This is what understands context and extracts information.

### Supabase / Postgres
Where you store the collected data. Every pipeline needs somewhere to save results.

We'll introduce these as needed—no need to learn them all upfront.

---

## Building Your First Scraper

Let's start simple: extracting information from a webpage.

### The Old Way (Regex)

Traditional scrapers use pattern matching to find data in HTML. This breaks whenever the website changes its layout:

```javascript
// Fragile—breaks if the HTML structure changes
const title = html.match(/<h1>(.*?)<\/h1>/)[1];
```

### The New Way (AI Extraction)

Instead of parsing HTML with code, let AI understand the content:

```typescript
const rawHtml = await page.content();
const prompt = `
  Extract all news headlines and dates from this HTML.
  Return as a JSON array: [{ headline: string, date: string }].
  HTML: ${rawHtml.substring(0, 10000)}...
`;
const result = await llm.complete(prompt);
```

What's happening:
1. Get the HTML from a page
2. Ask AI to find headlines and dates
3. AI understands the structure and extracts the data
4. You get clean, structured output

The advantage: even if the website changes its layout, AI can usually still find what you need.

---

## The Ethics of Scraping

Just because you can scrape something doesn't mean you should.

### Check robots.txt

Before scraping any site, visit `website.com/robots.txt`. This file tells you what the site owner allows bots to access. Respect it.

### Rate Limiting

Add delays between requests. Don't hammer servers with hundreds of requests per second. A 1-2 second delay between requests is reasonable.

### Identify Yourself

Set an honest User Agent:

```javascript
// Good: Honest identification
'User-Agent': 'TBS-Research-Bot/1.0 (research@tbsdigitallabs.com.au)'

// Bad: Pretending to be a regular browser
'User-Agent': 'Mozilla/5.0...'
```

---

## Practical Exercise: News Aggregator

Let's build a script that:
1. Visits a tech news site (like Hacker News)
2. Extracts the top 5 stories
3. Uses AI to summarise each in one sentence
4. Saves everything to JSON

### Setup

**Node.js:**
```bash
npm install puppeteer openai
```

**Python:**
```bash
pip install playwright openai
```

### The Steps

1. **Get the HTML** — Use Puppeteer/Playwright to load the page
2. **Extract headlines** — Send the HTML to AI with extraction instructions
3. **Summarise** — For each headline, ask AI for a one-sentence summary
4. **Save** — Write results to a JSON file

### Expected Output

```json
[
  {
    "headline": "New AI Tool Released",
    "link": "https://...",
    "summary": "A new AI tool that helps developers..."
  }
]
```

### Things to Consider

- What happens if the website is down?
- How do you handle errors gracefully?
- What if AI doesn't understand the HTML?
- How would you schedule this to run daily?

If you get stuck, that's part of learning. Try each step and see what breaks.

---

## Scaling Up

Once your first scraper works, the question becomes: how do I handle 100 websites?

Build incrementally:
1. Get one website working reliably
2. Add error handling and retries
3. Add a second website, reusing your code
4. Build the infrastructure (database, scheduling, monitoring)

Don't try to build everything at once.

---

## What's Next

You've built your first research tool. Coming up:

1. **AI-Assisted Testing** — Automate your test suite
2. **Refactoring** — Clean up and improve existing code
3. **Documentation** — Generate docs for your tools

---

## Resources

- [LangChain Documentation](https://js.langchain.com/docs/)
- [Puppeteer Guide](https://pptr.dev/)
- [Robots.txt Guide](https://developers.google.com/search/docs/crawling-indexing/robots/intro)

---

## Before Moving On

- [ ] Read through the content
- [ ] Built your first scraper (even a basic version)
- [ ] Understood the ethical guidelines
- [ ] Considered how you'd scale it

This should take about 60 minutes—longer if you're building along with the examples.
